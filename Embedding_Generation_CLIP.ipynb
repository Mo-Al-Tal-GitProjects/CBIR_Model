{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d74d8d75-a599-4db8-b170-54125a36ff44",
   "metadata": {},
   "source": [
    "## **Embedding Generation Using CLIP (Contrastive Language-Image Pre-training)**\n",
    "\n",
    "In this notebook, we will:\n",
    "- Load the pre-processed and augmented images.\n",
    "- Generate embeddings using the CLIP model.\n",
    "- Save the embeddings and related data for evaluation..\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbdbbef-c1ca-4cb6-9bab-f638dc878c8c",
   "metadata": {},
   "source": [
    "## **Table of Contents**\n",
    "---\n",
    "1. Import Libraries\n",
    "2. Set Up Device\n",
    "3. Load Augmented Image Mapping\n",
    "4. Load CLIP Model\n",
    "5. Generate Embeddings\n",
    "6. Select Representative Embeddings\n",
    "7. Save Embeddings\n",
    "8. Clear Memory\n",
    "9. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cccbba8-6c37-4452-8050-4278a262b005",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 1: Import Libraries**\n",
    "\n",
    "We begin by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a59abc5-063c-4e73-a12d-0a791d5110ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import gc\n",
    "from transformers import CLIPModel, CLIPProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f295b16-a4dd-4219-8e7c-09767a8e61df",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 2: Set Up Device**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f440f50e-95e2-4165-b408-6d57c5b16de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac17ee0e-fd17-495d-a4e2-162516be278b",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 3: Load Augmented Image Mapping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a519874-3d34-46cf-9fe1-0f01c127fa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load augmented image mapping\n",
    "augmented_df = pd.read_csv('augmented_image_mapping.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8087a78-3e8e-4ee9-bdd4-868cbe446ad7",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 4: Load CLIP Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a014959d-fa76-4ec9-9870-2b06aecdfcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohammedkhodorfirasal-tal/Documents/Professional/Work/Fellowship - Novartis/Data/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pre-trained CLIP model and processor\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "model = CLIPModel.from_pretrained(model_name).to(device)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfffd97b-f3ff-45b9-bf06-c0a4ba6b6920",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 5: Generate Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3be2b7bd-6e96-46c7-9016-474e91e95b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30240/30240 [29:15:28<00:00,  3.48s/it]\n"
     ]
    }
   ],
   "source": [
    "# Directory containing augmented images\n",
    "augmented_dir = './augmented_dataset/'\n",
    "\n",
    "# Prepare lists to store embeddings and image information\n",
    "embeddings = []\n",
    "augmented_image_files = []\n",
    "original_image_files = []\n",
    "\n",
    "# Generate embeddings\n",
    "for idx, row in tqdm(augmented_df.iterrows(), total=len(augmented_df), desc='Generating Embeddings'):\n",
    "    augmented_image_file = row['augmented_image']\n",
    "    original_image_file = row['original_image']\n",
    "    image_path = os.path.join(augmented_dir, augmented_image_file)\n",
    "\n",
    "    # Load and preprocess image\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    inputs = processor(images=img, return_tensors='pt').to(device)\n",
    "\n",
    "    # Generate embedding\n",
    "    with torch.no_grad():\n",
    "        outputs = model.get_image_features(**inputs)\n",
    "        embedding = outputs.cpu().numpy()[0]\n",
    "        # Normalize embedding\n",
    "        embedding = embedding / np.linalg.norm(embedding)\n",
    "        embeddings.append(embedding)\n",
    "        augmented_image_files.append(augmented_image_file)\n",
    "        original_image_files.append(original_image_file)\n",
    "\n",
    "    # Clear memory\n",
    "    del img, inputs, outputs\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Convert embeddings to NumPy array\n",
    "embeddings = np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b44ea8d-bcb6-430d-bd6e-2b8dfd30c6c9",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 6: Select Representative Embeddings**\n",
    "\n",
    "We will use the Highest Norm Criterion to select the most representative embedding for each original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ca509a1-7a4c-406c-bd44-4b29b233cf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for grouping\n",
    "data = pd.DataFrame({\n",
    "    'original_image': original_image_files,\n",
    "    'augmented_image': augmented_image_files,\n",
    "    'embedding_index': range(len(embeddings))\n",
    "})\n",
    "\n",
    "selected_embeddings = []\n",
    "selected_image_files = []\n",
    "\n",
    "# Group by original image and select the embedding with the highest norm\n",
    "for original_image, group in data.groupby('original_image'):\n",
    "    indices = group['embedding_index'].tolist()\n",
    "    group_embeddings = embeddings[indices]\n",
    "    # Compute norms of embeddings\n",
    "    norms = np.linalg.norm(group_embeddings, axis=1)\n",
    "    # Select the embedding with the highest norm\n",
    "    best_idx_in_group = np.argmax(norms)\n",
    "    best_idx = indices[best_idx_in_group]\n",
    "    selected_embeddings.append(embeddings[best_idx])\n",
    "    selected_image_files.append(original_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34d25f4-6515-4759-a9ca-d277a863fca9",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 7: Save Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a88be217-37c3-4214-990f-31842ad87070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings for CLIP saved.\n"
     ]
    }
   ],
   "source": [
    "# Convert selected embeddings to NumPy array\n",
    "selected_embeddings = np.vstack(selected_embeddings)\n",
    "\n",
    "# Save embeddings and image files\n",
    "np.save('embeddings_clip.npy', selected_embeddings)\n",
    "np.save('image_files_clip.npy', selected_image_files)\n",
    "\n",
    "print('Embeddings for CLIP saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1522ee72-849f-4c2b-92f6-a6f67aa662c7",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 8: Clear Memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adebd445-bcb6-441a-a680-d08c646c080a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear variables and free memory\n",
    "del embeddings, augmented_image_files, original_image_files, data\n",
    "del selected_embeddings, selected_image_files, model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f283ced-acf5-4134-a219-06cdbf9b4d2e",
   "metadata": {},
   "source": [
    "---\n",
    "### **Conclusion**\n",
    "\n",
    "We have generated embeddings using the CLIP model, selected representative embeddings, and saved them for evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
