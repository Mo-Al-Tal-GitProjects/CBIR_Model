{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81cf44dc-374f-4af2-8cfa-b5ebb9af935e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Embedding Generation Using UNI (Unified Image Embedding Model)**\n",
    "\n",
    "In this notebook, we will:\n",
    "- Load the pre-processed and augmented images.\n",
    "- Generate embeddings using the UNI model.\n",
    "- Save the embeddings and related data for evaluation..\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fda2782-4fb3-46f3-954c-bcbf818643bb",
   "metadata": {},
   "source": [
    "## **Table of Contents**\n",
    "---\n",
    "1. Import Libraries\n",
    "2. Set Up Device\n",
    "3. Load Augmented Image Mapping\n",
    "4. Load UNI Model\n",
    "5. Generate Embeddings\n",
    "6. Select Representative Embeddings\n",
    "7. Save Embeddings\n",
    "8. Clear Memory\n",
    "9. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9ceb10-51f8-44eb-b050-662b0abde87b",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 1: Import Libraries**\n",
    "\n",
    "We begin by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "640b5dfe-b53f-4c8f-ad07-639a2c1cb461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import gc\n",
    "\n",
    "# Add UNI directory to sys.path\n",
    "sys.path.append('./UNI')\n",
    "\n",
    "# Import the necessary modules from UNI\n",
    "from uni.get_encoder import get_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1667b800-0a4d-4c38-8c14-75d3a01ad460",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 2: Set Up Device**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c0bb6e7-8423-4c29-9e8c-b40f7b9f3e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc31ba86-9f1a-4b9c-9feb-18367a7058db",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 3: Load Augmented Image Mapping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f02f14f6-99be-428a-b247-da9a45c9e89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load augmented image mapping\n",
    "augmented_df = pd.read_csv('augmented_image_mapping.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0282b9b2-b40c-495b-930e-7cff9e200332",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 4: Load UNI Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dd38d67-dc9d-4daf-93ff-c4db088681ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Load UNI Model\n",
    "\n",
    "# Load the pre-trained UNI model\n",
    "checkpoint = 'pytorch_model.bin'\n",
    "\n",
    "# Use the get_encoder function to load the model and the transform\n",
    "model, transform = get_encoder(\n",
    "    enc_name='vit_large_patch16_224.dinov2.uni_mass100k',\n",
    "    checkpoint=checkpoint,\n",
    "    which_img_norm='imagenet',\n",
    "    img_resize=224,\n",
    "    center_crop=False,\n",
    "    test_batch=0,\n",
    "    device=device,\n",
    "    assets_dir=assets_dir,\n",
    "    kwargs={},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e740d034-095e-4968-9fcf-56eb010a573a",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 5: Generate Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b1c7ffb-06a0-4cc5-a218-73cbd745b121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 180/180 [4:58:52<00:00, 99.63s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 168  # Adjust based on your GPU memory\n",
    "embeddings = []\n",
    "original_image_files = []\n",
    "\n",
    "# Directory containing augmented images\n",
    "augmented_dir = './augmented_dataset/'\n",
    "\n",
    "# Prepare a list of all image paths and corresponding original images\n",
    "image_paths = augmented_df['augmented_image'].apply(lambda x: os.path.join(augmented_dir, x)).tolist()\n",
    "original_images = augmented_df['original_image'].tolist()\n",
    "\n",
    "# Total number of images\n",
    "total_images = len(image_paths)\n",
    "\n",
    "# Generate embeddings in batches\n",
    "for start_idx in tqdm(range(0, total_images, batch_size), desc='Generating Embeddings'):\n",
    "    end_idx = min(start_idx + batch_size, total_images)\n",
    "    batch_image_paths = image_paths[start_idx:end_idx]\n",
    "    batch_original_images = original_images[start_idx:end_idx]\n",
    "\n",
    "    # Load and preprocess images\n",
    "    imgs = [Image.open(p).convert('RGB') for p in batch_image_paths]\n",
    "    img_tensors = torch.stack([transform(img) for img in imgs]).to(device)\n",
    "\n",
    "    # Generate embeddings\n",
    "    with torch.no_grad():\n",
    "        batch_embeddings = model(img_tensors)\n",
    "        batch_embeddings = batch_embeddings.cpu().numpy()\n",
    "        # Normalize embeddings\n",
    "        batch_embeddings = batch_embeddings / np.linalg.norm(batch_embeddings, axis=1, keepdims=True)\n",
    "        embeddings.extend(batch_embeddings)\n",
    "        original_image_files.extend(batch_original_images)\n",
    "\n",
    "    # Clear memory\n",
    "    del imgs, img_tensors, batch_embeddings\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ed4b54-b6a4-4df7-ab67-d94996d81e3a",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 6: Select Representative Embeddings**\n",
    "\n",
    "We will use the Highest Norm Criterion to select the most representative embedding for each original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82d9a2a5-68c6-45b7-8bd1-b14157c5e2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert embeddings to NumPy array\n",
    "embeddings = np.vstack(embeddings)\n",
    "\n",
    "# Create a DataFrame for grouping\n",
    "data = pd.DataFrame({\n",
    "    'original_image': augmented_df['original_image'],\n",
    "    'embedding_index': range(len(embeddings))\n",
    "})\n",
    "\n",
    "selected_embeddings = []\n",
    "selected_image_files = []\n",
    "\n",
    "# Group by original image and select the embedding with the highest norm\n",
    "for original_image, group in data.groupby('original_image'):\n",
    "    indices = group['embedding_index'].tolist()\n",
    "    group_embeddings = embeddings[indices]\n",
    "    # Compute norms of embeddings\n",
    "    norms = np.linalg.norm(group_embeddings, axis=1)\n",
    "    # Select the embedding with the highest norm\n",
    "    best_idx_in_group = np.argmax(norms)\n",
    "    best_idx = indices[best_idx_in_group]\n",
    "    selected_embeddings.append(embeddings[best_idx])\n",
    "    selected_image_files.append(original_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50101c0e-5600-413d-88e2-04aec4f0841e",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 7: Save Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23fa0b51-b9d0-4468-9126-5eb31fd6bce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings for UNI saved.\n"
     ]
    }
   ],
   "source": [
    "# Convert selected embeddings to NumPy array\n",
    "selected_embeddings = np.vstack(selected_embeddings)\n",
    "\n",
    "# Save embeddings and image files\n",
    "np.save('embeddings_uni.npy', selected_embeddings)\n",
    "np.save('image_files_uni.npy', selected_image_files)\n",
    "\n",
    "print('Embeddings for UNI saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e598fd-e3aa-4593-a194-915a24fd8a9a",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 8: Clear Memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3671d8e3-d4b9-4655-8f05-1cc27e03b042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear variables and free memory\n",
    "del embeddings, augmented_image_files, original_image_files, data\n",
    "del selected_embeddings, selected_image_files, model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae3ca73-2568-461e-aca6-e1f0b3c3825d",
   "metadata": {},
   "source": [
    "---\n",
    "### **Conclusion**\n",
    "\n",
    "We have generated embeddings using the UNI model, selected representative embeddings, and saved them for evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
