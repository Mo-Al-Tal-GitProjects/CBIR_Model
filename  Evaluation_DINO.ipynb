{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fdd2af5-6403-4848-98a9-29a1a09463c5",
   "metadata": {},
   "source": [
    "## **Evaluation of Image Retrieval Using DINO Embeddings**\n",
    "\n",
    "In this notebook, we will:\n",
    "- Load the embeddings generated using the DINO model.\n",
    "- Use FAISS for similarity search to perform image retrieval.\n",
    "- Evaluate the retrieval performance using appropriate metrics.\n",
    "- Visualize the retrieval results.\n",
    "- Create an evaluation DataFrame summarizing the results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0051d1f1-f4a1-48f7-a6bd-06733035080c",
   "metadata": {},
   "source": [
    "## **Table of Contents**\n",
    "---\n",
    "\n",
    "1. Import Libraries\n",
    "2. Load Embeddings and Image Files\n",
    "3. Load Augmented Image Mapping and Captions\n",
    "4. Prepare the Dataset\n",
    "5. Set Up Similarity Search (FAISS)\n",
    "6. Define Functions for Retrieval and Evaluation\n",
    "7. Perform Image Retrieval\n",
    "8. Cluster Embeddings\n",
    "9. Compute Unsupervised Evaluation Metrics\n",
    "10. Visualize Sample Images from Each Cluster\n",
    "11. Create Evaluation DataFrame\n",
    "12. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7508ec69-518a-4511-9309-ff77aa1786ff",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 1: Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4512e2-0745-4acd-8842-f94397166ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a43cafe-ee37-4a7b-b521-035d3b2e9d21",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 2: Load Embeddings and Image Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b114eaa-23d0-4c43-8f92-4cc9f28e4cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings and image files\n",
    "embeddings = np.load('embeddings_dino.npy')\n",
    "image_files = np.load('image_files_dino.npy')\n",
    "\n",
    "print(f'Embeddings shape: {embeddings.shape}')\n",
    "print(f'Number of images: {len(image_files)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c086354c-5488-4e0a-a7d6-8f2bba850401",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 3: Load Augmented Image Mapping and Captions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ebc25b-a3eb-4049-b81b-6fc941741f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load augmented image mapping\n",
    "augmented_df = pd.read_csv('augmented_image_mapping.csv')\n",
    "\n",
    "# Load captions from pubmed_set\n",
    "captions_file = './pubmed_set/captions.json'\n",
    "with open(captions_file, 'r') as f:\n",
    "    captions_data = json.load(f)\n",
    "\n",
    "# Create a mapping from UUID to caption\n",
    "uuid_to_caption = {entry['uuid']: entry['caption'] for entry in captions_data.values()}\n",
    "\n",
    "# Map filenames to captions for pubmed_set images\n",
    "pubmed_image_captions = {}\n",
    "pubmed_images_dir = './pubmed_set/images/'\n",
    "for filename in os.listdir(pubmed_images_dir):\n",
    "    uuid = os.path.splitext(filename)[0]\n",
    "    if uuid in uuid_to_caption:\n",
    "        pubmed_image_captions[filename] = uuid_to_caption[uuid]\n",
    "\n",
    "# Convert the pubmed captions mapping into a DataFrame\n",
    "captions_df = pd.DataFrame(\n",
    "    list(pubmed_image_captions.items()), columns=['image_file', 'caption']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fa3c42-6dfa-4739-962d-aeb7b3115528",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 4: Prepare the Dataset**\n",
    "\n",
    "We create a DataFrame that includes image file names, their source dataset, and any associated captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da604ee1-d286-4da4-a56e-50b95d12542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for image files\n",
    "df = pd.DataFrame({\n",
    "    'image_file': image_files,\n",
    "    'embedding_index': range(len(image_files))\n",
    "})\n",
    "\n",
    "# Create a mapping from augmented to original images\n",
    "augmented_mapping = augmented_df.set_index('augmented_image')['original_image'].to_dict()\n",
    "\n",
    "# Apply the mapping to determine the original image\n",
    "df['original_image'] = df['image_file'].apply(lambda x: augmented_mapping.get(x, x))\n",
    "\n",
    "# Determine the source dataset for each image\n",
    "def determine_source(image_name):\n",
    "    # Matches the format XXX_XX.jpg where XXX and XX are digits\n",
    "    if re.match(r'^\\d{3}_\\d{2}\\.jpg$', image_name):\n",
    "        return 'matches'\n",
    "    # Matches the format XXXX_XX.jpg where XXXX and XX are digits\n",
    "    elif re.match(r'^\\d{4}_\\d{2}\\.jpg$', image_name):\n",
    "        return 'matches'\n",
    "    else:\n",
    "        return 'pubmed'\n",
    "\n",
    "# Apply the source determination function\n",
    "df['source'] = df['image_file'].apply(determine_source)\n",
    "\n",
    "# Merge captions for pubmed images\n",
    "df = df.merge(captions_df, on='image_file', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9689be82-0f3d-46dc-a56f-a5cb33850afc",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 5: Set Up Similarity Search with FAISS**\n",
    "\n",
    "FAISS (Facebook AI Similarity Search) is a library for efficient similarity search and clustering of dense vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4aa37cd-a8c1-46a7-a986-19fc1c348514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimension of the embeddings\n",
    "d = embeddings.shape[1]\n",
    "\n",
    "# Build the index\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(embeddings)\n",
    "\n",
    "print(f'Number of vectors in the index: {index.ntotal}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b184b2e3-66b2-4d7c-88e9-6f1f1f3ae642",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 6: Define Functions for Retrieval and Evaluation**\n",
    "\n",
    "Function to Display Images with Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c946a4-79cd-472d-849b-67756d56ce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(image_paths, titles=None, captions=None, cols=1, figsize=(60, 32)):\n",
    "    rows = len(image_paths) // cols + int(len(image_paths) % cols != 0)\n",
    "    plt.figure(figsize=figsize)\n",
    "    for idx, img_path in enumerate(image_paths):\n",
    "        plt.subplot(rows, cols, idx+1)\n",
    "        img = Image.open(img_path)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        title = titles[idx] if titles else ''\n",
    "        caption = captions[idx] if captions else ''\n",
    "        full_title = f\"{title}\\n{caption}\"\n",
    "        plt.title(full_title, fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e95bbb-d58e-4184-bf14-ed5f2f369cc5",
   "metadata": {},
   "source": [
    "Function to Retrieve and Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d6664f-8c56-43f0-8fbb-87aa8d16e3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_display(query_image_name, top_k=10):\n",
    "    # Get the index of the query image\n",
    "    query_indices = df[df['original_image'] == query_image_name]['embedding_index'].tolist()\n",
    "    if not query_indices:\n",
    "        print(f\"No embeddings found for query image: {query_image_name}\")\n",
    "        return\n",
    "    \n",
    "    # Use the first embedding as the query\n",
    "    query_idx = query_indices[0]\n",
    "    query_embedding = embeddings[query_idx].reshape(1, -1)\n",
    "    \n",
    "    # Perform the search\n",
    "    distances, indices = index.search(query_embedding, top_k)  # Remove +1 to only retrieve top_k results\n",
    "    \n",
    "    # Get retrieved images (including the query image itself if present)\n",
    "    retrieved_indices = indices[0]\n",
    "    retrieved_images = df.iloc[retrieved_indices]\n",
    "    \n",
    "    # Check if the query image is already included in the retrieval\n",
    "    if query_idx not in retrieved_indices:\n",
    "        # Manually add the query image to the top of the results\n",
    "        query_row = df.iloc[[query_idx]]\n",
    "        retrieved_images = pd.concat([query_row, retrieved_images]).drop_duplicates().head(top_k)\n",
    "    \n",
    "    # Prepare paths and titles\n",
    "    query_image_file = df.loc[query_idx, 'image_file']\n",
    "    query_image_path = os.path.join('./combined_dataset/', query_image_file)\n",
    "    retrieved_image_paths = [os.path.join('./combined_dataset/', img_file) for img_file in retrieved_images['image_file']]\n",
    "    titles = [f\"Rank {i+1}: {img_name}\" for i, img_name in enumerate(retrieved_images['image_file'])]\n",
    "    captions = []\n",
    "    for _, row in retrieved_images.iterrows():\n",
    "        if row['source'] == 'pubmed':\n",
    "            captions.append(row['caption'])\n",
    "        else:\n",
    "            captions.append('')\n",
    "    \n",
    "    # Combine query image and retrieved images for uniform display\n",
    "    all_image_paths = [query_image_path] + retrieved_image_paths\n",
    "    all_titles = [f\"Query: {query_image_file}\"] + titles\n",
    "    all_captions = [None] + captions  # Query image may not have a caption\n",
    "    \n",
    "    # Dynamically adjust figure size based on number of images\n",
    "    total_images = len(all_image_paths)\n",
    "    figsize = (15, total_images * 3)  # Each image gets 3 units of vertical space\n",
    "\n",
    "    # Display the query and retrieved images together\n",
    "    print(\"Displaying Query and Retrieved Images:\")\n",
    "    display_images(all_image_paths, titles=all_titles, captions=all_captions, cols=1, figsize=figsize)\n",
    "    \n",
    "    return retrieved_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337ad405-4822-43b5-ab25-a2a65cc95d11",
   "metadata": {},
   "source": [
    "Function to Evaluate Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cc83dd-69b0-4253-8438-111ca8a43eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval(query_image_name, retrieved_images):\n",
    "    # Get all available matches for the query image\n",
    "    total_available_matches = len(df[(df['original_image'].str.startswith(query_image_name.split('_')[0])) & (df['source'] == 'matches')])\n",
    "    \n",
    "    # Count the number of matches in the retrieved images\n",
    "    retrieved_matches = retrieved_images[retrieved_images['original_image'].str.startswith(query_image_name.split('_')[0])]\n",
    "    num_matches_found = len(retrieved_matches[retrieved_matches['source'] == 'matches'])\n",
    "    \n",
    "    # Calculate match accuracy\n",
    "    if total_available_matches > 0:\n",
    "        match_accuracy = num_matches_found / total_available_matches\n",
    "    else:\n",
    "        match_accuracy = 0\n",
    "    \n",
    "    return num_matches_found, total_available_matches, match_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb6e7b1-8a53-4544-b9e8-9a73ea6ff7e8",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 7: Perform Image Retrieval**\n",
    "\n",
    "We will perform a k-nearest neighbors search for each image in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfbe6cd-e8af-47b4-b1db-077c305726fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Perform Image Retrieval\n",
    "\n",
    "# Get all unique original images from the matches set\n",
    "matches_images = df[df['source'] == 'matches']['original_image'].unique()\n",
    "\n",
    "# Display available images\n",
    "if len(matches_images) > 0:\n",
    "    print(\"Available images for querying from the 'matches' set:\")\n",
    "    for idx, image_file in enumerate(matches_images):\n",
    "        print(f\"{idx}: {image_file}\")\n",
    "\n",
    "    # Prompt the user to select an image\n",
    "    while True:\n",
    "        try:\n",
    "            query_idx = int(input(f\"Enter the index of the query image (0 to {len(matches_images) - 1}): \"))\n",
    "            if 0 <= query_idx < len(matches_images):\n",
    "                query_image_name = matches_images[query_idx]\n",
    "                print(f\"Selected query image: {query_image_name}\")\n",
    "                \n",
    "                # Retrieve and display images\n",
    "                print(\"\\nRetrieving images...\")\n",
    "                retrieved_images = retrieve_and_display(query_image_name, top_k=10)\n",
    "                \n",
    "                if retrieved_images is not None:\n",
    "                    # Evaluate the retrieval\n",
    "                    print(\"\\nEvaluating retrieval performance...\")\n",
    "                    num_matches_found, total_available_matches, match_accuracy = evaluate_retrieval(query_image_name, retrieved_images)\n",
    "                    \n",
    "                    # Display evaluation results\n",
    "                    print(\"\\nRetrieval Evaluation Results:\")\n",
    "                    print(f\"Total Matches Available: {total_available_matches}\")\n",
    "                    print(f\"Matches Found in Retrieved Images: {num_matches_found}\")\n",
    "                    print(f\"Match Accuracy: {match_accuracy:.2f}\")\n",
    "                else:\n",
    "                    print(\"No images retrieved.\")\n",
    "                \n",
    "                break  # Exit the loop after processing\n",
    "            else:\n",
    "                print(f\"Invalid index. Please enter a number between 0 and {len(matches_images) - 1}.\")\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a valid integer.\")\n",
    "else:\n",
    "    print(\"No images available in the 'matches' set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95ef393-968c-4805-abe1-d33914bc0f93",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 8: Cluster Embeddings**\n",
    "\n",
    "We can use clustering algorithms like K-Means to group similar images and evaluate the cohesiveness of these clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfe535c-8551-4a55-b42c-a815d4e7df0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the number of clusters (dynamic based on dataset size)\n",
    "num_clusters = min(10, len(embeddings) // 100)  # For example, 1 cluster per 100 images or at least 10 clusters\n",
    "\n",
    "# Initialize K-Means\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "\n",
    "# Fit K-Means on the embeddings\n",
    "cluster_labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# Add cluster labels to the DataFrame\n",
    "df['cluster_label'] = cluster_labels\n",
    "\n",
    "print(f\"Clustering completed with {num_clusters} clusters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b3c9ba-3253-40cc-bcf4-a39df9df1920",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 9: Compute Unsupervised Evaluation Metrics**\n",
    "\n",
    "**Silhouette Score:** measures how similar an object is to its own cluster compared to other clusters. It ranges from -1 to 1, with higher values indicating better clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1575e0-2954-4015-9e02-7f40cc5efb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Silhouette Score\n",
    "if num_clusters > 1:  # Silhouette Score is valid only if there are >1 clusters\n",
    "    sil_score = silhouette_score(embeddings, cluster_labels)\n",
    "    print(f'Silhouette Score: {sil_score:.4f}')\n",
    "else:\n",
    "    print(\"Silhouette Score cannot be computed with a single cluster.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e75aeb9-592d-448d-9187-3d5f898d1933",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 10: Visualize Sample Images from Each Cluster**\n",
    "\n",
    "Visualize Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a488653-cc2e-4aba-9ddf-7e3d8c4d72f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display images from clusters\n",
    "def display_cluster_images(cluster_num, images_per_cluster=5):\n",
    "    \"\"\"\n",
    "    Display a sample of images from a specified cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - cluster_num: The cluster number to display images from.\n",
    "    - images_per_cluster: The number of images to sample and display.\n",
    "    \"\"\"\n",
    "    cluster_images = df[df['cluster_label'] == cluster_num]['image_file'].values\n",
    "    if len(cluster_images) == 0:\n",
    "        print(f\"Cluster {cluster_num} is empty.\")\n",
    "        return\n",
    "    sample_images = np.random.choice(cluster_images, size=min(images_per_cluster, len(cluster_images)), replace=False)\n",
    "    sample_image_paths = [os.path.join('./combined_dataset/', img_file) for img_file in sample_images]\n",
    "    titles = [f\"Cluster {cluster_num}: {img_file}\" for img_file in sample_images]\n",
    "    display_images(sample_image_paths, titles=titles, figsize=(30, 16), cols=3)\n",
    "\n",
    "# Visualize images from each cluster\n",
    "for cluster in range(num_clusters):\n",
    "    print(f\"\\nCluster {cluster} (Total Images: {len(df[df['cluster_label'] == cluster])}):\")\n",
    "    display_cluster_images(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce002cc-7919-4546-ba2a-c1b2ff0276d2",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 11: Create Evaluation DataFrame**\n",
    "\n",
    "Evaluate Over All Images in the Matches Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f709b38c-8fcf-4cb0-ad1c-5feaea8c8fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a list to store evaluation results\n",
    "evaluation_results = []\n",
    "\n",
    "# Loop through each query image and evaluate\n",
    "for query_image_name in tqdm(matches_images, desc='Evaluating Queries'):\n",
    "    # Retrieve images without displaying\n",
    "    query_indices = df[df['original_image'] == query_image_name]['embedding_index'].tolist()\n",
    "    if not query_indices:\n",
    "        print(f\"No embeddings found for query image: {query_image_name}\")\n",
    "        continue\n",
    "\n",
    "    # Use the first embedding as the query\n",
    "    query_idx = query_indices[0]\n",
    "    query_embedding = embeddings[query_idx].reshape(1, -1)\n",
    "    \n",
    "    # Perform the search\n",
    "    distances, indices = index.search(query_embedding, 10)  # Retrieve top 10 results\n",
    "    retrieved_indices = indices[0]\n",
    "    retrieved_images = df.iloc[retrieved_indices]\n",
    "    \n",
    "    # Evaluate retrieval\n",
    "    num_matches_found, total_available_matches, match_accuracy = evaluate_retrieval(query_image_name, retrieved_images)\n",
    "    \n",
    "    # Store the results\n",
    "    evaluation_results.append({\n",
    "        'Queried Image': query_image_name,\n",
    "        'Number of Matches Found': num_matches_found,\n",
    "        'Total Available Matches': total_available_matches,\n",
    "        'Match Accuracy': match_accuracy\n",
    "    })\n",
    "    \n",
    "    # Clear memory\n",
    "    gc.collect()\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "evaluation_df = pd.DataFrame(evaluation_results)\n",
    "\n",
    "# Display the evaluation DataFrame\n",
    "evaluation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c24718-2bc6-43a1-b59c-7c3e0027f562",
   "metadata": {},
   "source": [
    "Create DataFrame and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0762e223-45e9-4fbe-9e71-6ef6520fd59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "evaluation_df.to_csv('evaluation_results_dino.csv', index=False)\n",
    "\n",
    "print('Evaluation results saved to evaluation_results_dino.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552c91bd-1d3c-4b5b-93f2-7598b22cd405",
   "metadata": {},
   "source": [
    "### **Step 12: Conclusion**\n",
    "\n",
    "We have successfully:\n",
    "\n",
    "- Loaded the embeddings generated using the PLIP model.\n",
    "- Performed image retrieval using FAISS without labels.\n",
    "- Visualized retrieval results to qualitatively assess similarity.\n",
    "- Applied K-Means clustering on the embeddings.\n",
    "- Computed the Silhouette Score to evaluate clustering quality.\n",
    "- Visualized sample images from each cluster.\n",
    "- Created an evaluation DataFrame summarizing the results for all query images."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
