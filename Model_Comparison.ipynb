{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9292e9ee-b03e-4db9-b1fc-f5c3040c593f",
   "metadata": {},
   "source": [
    "## **Comparative Analysis of Pre-trained Models for Image Retrieval in Computational Pathology**\n",
    "\n",
    "\n",
    "In this notebook, we will:\n",
    "- Aggregate and compare evaluation results from four pre-trained models: PLIP, UNI, CLIP, DINO.\n",
    "- Visualize and analyze the embeddings and retrieval results.\n",
    "- Highlight \"match sets\" in t-SNE plots to assess how models cluster similar images.\n",
    "- Draw conclusions about the relative performance of each model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82ae0ea-cfd3-41d6-8eb6-2f30d295894d",
   "metadata": {},
   "source": [
    "## **Table of Contents**\n",
    "---\n",
    "\n",
    "1. Import Libraries\n",
    "2. Load Evaluation Data\n",
    "3. Create Master Evaluation DataFrame\n",
    "4. Visualize Model Performance\n",
    "5. Visualize Embeddings Using t-SNE with Highlighted Matches\n",
    "6. Compare Retrieval Results Across Models\n",
    "7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78098c5-374a-4cb1-8176-3c1aa4486ed8",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 1: Import Libraries**\n",
    "\n",
    "We begin by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3961c163-fad2-4096-88b8-976936f4756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.manifold import TSNE\n",
    "import faiss\n",
    "import random\n",
    "import gc\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55109630-1d36-4e20-b922-600e40c6e0bb",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 2: Load Evaluation Data**\n",
    "\n",
    "We'll load the embeddings and any evaluation data for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4d5ec1-f65d-4007-8ac3-9068c3cf7f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models\n",
    "models = ['PLIP', 'UNI', 'DINO', 'CLIP']\n",
    "\n",
    "# Directory paths\n",
    "embeddings_dir = './'  # Adjust if embeddings are stored elsewhere\n",
    "evaluation_dir = './'  # Adjust if evaluation results are stored elsewhere\n",
    "combined_dataset_dir = './combined_dataset/'  # Directory containing images\n",
    "\n",
    "# Dictionary to hold model data\n",
    "model_data = {}\n",
    "\n",
    "for model in models:\n",
    "    # Load embeddings and image files\n",
    "    embeddings = np.load(os.path.join(embeddings_dir, f'embeddings_{model.lower()}.npy'))\n",
    "    image_files = np.load(os.path.join(embeddings_dir, f'image_files_{model.lower()}.npy'))\n",
    "    \n",
    "    # Load evaluation results\n",
    "    evaluation_df = pd.read_csv(os.path.join(evaluation_dir, f'evaluation_results_{model.lower()}.csv'))\n",
    "    \n",
    "    # Load cluster labels if available\n",
    "    cluster_labels = None\n",
    "    cluster_labels_file = os.path.join(evaluation_dir, f'cluster_labels_{model.lower()}.npy')\n",
    "    if os.path.exists(cluster_labels_file):\n",
    "        cluster_labels = np.load(cluster_labels_file)\n",
    "    \n",
    "    # Store in the dictionary\n",
    "    model_data[model] = {\n",
    "        'embeddings': embeddings,\n",
    "        'image_files': image_files,\n",
    "        'evaluation_df': evaluation_df,\n",
    "        'cluster_labels': cluster_labels\n",
    "    }\n",
    "    \n",
    "    print(f'{model}:')\n",
    "    print(f'  Embeddings shape: {embeddings.shape}')\n",
    "    print(f'  Number of images: {len(image_files)}')\n",
    "    print(f'  Evaluation results loaded with shape: {evaluation_df.shape}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787c309d-9736-4ccf-964b-b0ae43208387",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 3: Create Master Evaluation DataFrame**\n",
    "\n",
    "We will merge the evaluation results from all models into a single DataFrame for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ac212b-b330-43b7-a390-bc15c1b73ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with the list of queried images and total available matches from one model (assuming they are the same across models)\n",
    "queried_images = model_data['PLIP']['evaluation_df']['Queried Image']\n",
    "total_available_matches = model_data['PLIP']['evaluation_df']['Total Available Matches']\n",
    "\n",
    "# Initialize the master DataFrame\n",
    "master_df = pd.DataFrame({\n",
    "    'Queried Image': queried_images,\n",
    "    'Total Available Matches': total_available_matches\n",
    "})\n",
    "\n",
    "# Add match counts and accuracies from each model\n",
    "for model in models:\n",
    "    eval_df = model_data[model]['evaluation_df']\n",
    "    master_df[f'Matches Found ({model})'] = eval_df['Number of Matches Found']\n",
    "    master_df[f'Match Accuracy ({model})'] = eval_df['Match Accuracy']\n",
    "\n",
    "# Display the master DataFrame\n",
    "master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e482e489-38c3-4a35-b018-27f8edbb4df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# Number of queried images to plot\n",
    "queried_images = plot_data['Queried Image'].unique()\n",
    "num_images = len(queried_images)\n",
    "\n",
    "# Create subplots\n",
    "num_cols = 3\n",
    "num_rows = math.ceil(num_images / num_cols)\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 5), sharey=True)\n",
    "\n",
    "# Flatten axes for easier indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each queried image in a separate subplot\n",
    "for i, queried_image in enumerate(queried_images):\n",
    "    subset = plot_data[plot_data['Queried Image'] == queried_image]\n",
    "    axes[i].bar(subset['Model'], subset['Accuracy'], color='skyblue', alpha=0.8)\n",
    "    axes[i].set_title(queried_image, fontsize=10)\n",
    "    axes[i].set_xlabel('Models', fontsize=9)\n",
    "    axes[i].set_ylabel('Accuracy', fontsize=9)\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Remove extra subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Model Accuracies for Top 15 Queried Images', fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccb0d8d-ccb0-4db4-9bfa-15f4949937cc",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 4: Visualize Model Performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fd7b98-e469-4206-90db-0a519ef98e98",
   "metadata": {},
   "source": [
    "**4.1. Plot Heatmap of Match Accuracies**\n",
    "\n",
    "We will visualize the match accuracies of different models for each queried image using a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a251468-6fd8-439a-86fa-73c1b5a7cbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for heatmap\n",
    "accuracy_columns = [f'Match Accuracy ({model})' for model in models]\n",
    "heatmap_data = master_df[['Queried Image'] + accuracy_columns].set_index('Queried Image')\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".2f\", cmap='viridis')\n",
    "plt.title('Match Accuracies Across Models and Queried Images')\n",
    "plt.ylabel('Queried Image')\n",
    "plt.xlabel('Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5099d52e-491c-48b9-9e56-ca634bb8dc7f",
   "metadata": {},
   "source": [
    "**4.2. Plot Box Plots of Match Accuracies**\n",
    "\n",
    "We will use box plots to compare the distribution of match accuracies for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f54c4c-bfa4-4cc9-85ec-dccbaa9f86a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for box plots\n",
    "accuracy_data = pd.melt(master_df, id_vars=['Queried Image'], value_vars=accuracy_columns,\n",
    "                        var_name='Model', value_name='Match Accuracy')\n",
    "accuracy_data['Model'] = accuracy_data['Model'].str.extract(r'\\((.*?)\\)')\n",
    "\n",
    "# Create box plots\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Model', y='Match Accuracy', data=accuracy_data)\n",
    "plt.title('Distribution of Match Accuracies per Model')\n",
    "plt.ylabel('Match Accuracy')\n",
    "plt.xlabel('Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8442ac-011d-4bd1-a0ff-e3b4acc33da8",
   "metadata": {},
   "source": [
    "**4.3. Plot Line Plots of Match Accuracies**\n",
    "\n",
    "We will plot match accuracies per model across queried images to observe trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ad2944-dd75-4a74-96fd-a244c0a25a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort master_df by total available matches\n",
    "sorted_df = master_df.sort_values('Total Available Matches')\n",
    "\n",
    "# Plot match accuracies\n",
    "plt.figure(figsize=(14, 6))\n",
    "for model in models:\n",
    "    plt.plot(sorted_df['Queried Image'], sorted_df[f'Match Accuracy ({model})'], marker='o', label=model)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Queried Image')\n",
    "plt.ylabel('Match Accuracy')\n",
    "plt.title('Match Accuracy per Model Across Queried Images')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487aca33-950c-4a26-aa31-d25a80d996c1",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 5:  Visualize Embeddings Using t-SNE with Highlighted Matches**\n",
    "\n",
    "We will create t-SNE plots for each model, highlighting \"match sets\" within clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66678ef-fe74-42f0-a7aa-2a3c80218916",
   "metadata": {},
   "source": [
    "**5.1. Prepare Data for t-SNE Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6b7423-e99f-4329-b329-853dd2899a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping of \"match sets\"\n",
    "def get_match_set(image_name):\n",
    "    # Extract the prefix before the underscore (e.g., '387' from '387_01.jpg')\n",
    "    match = re.match(r'^(\\d+)_\\d+\\.jpg$', image_name)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return 'pubmed'\n",
    "\n",
    "# For each model, prepare the DataFrame with embeddings and labels\n",
    "for model in models:\n",
    "    embeddings = model_data[model]['embeddings']\n",
    "    image_files = model_data[model]['image_files']\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'image_file': image_files,\n",
    "        'embedding_index': range(len(image_files))\n",
    "    })\n",
    "    \n",
    "    df['match_set'] = df['image_file'].apply(get_match_set)\n",
    "    model_data[model]['df'] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac724fb-e868-4fbd-9efc-67b3b7796fc0",
   "metadata": {},
   "source": [
    "**5.2. Compute t-SNE Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18cf903-c112-4ac9-a95c-ea977f484565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute t-SNE embeddings for each model\n",
    "for model in models:\n",
    "    print(f'Computing t-SNE for {model}...')\n",
    "    embeddings = model_data[model]['embeddings']\n",
    "    \n",
    "    # Reduce dimensions to 2D\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=500)\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "    model_data[model]['embeddings_2d'] = embeddings_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9683e59-e02f-4361-a86c-1c23207f5b35",
   "metadata": {},
   "source": [
    "**5.3. Visualize t-SNE Plots with Highlighted Match Sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5087d0-5d18-4212-90d5-fa3df4fbdda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, perform K-Means clustering on the embeddings\n",
    "num_clusters = 10  # Adjust based on your dataset size\n",
    "for model in models:\n",
    "    embeddings = model_data[model]['embeddings']\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    model_data[model]['cluster_labels'] = cluster_labels\n",
    "\n",
    "# Updated plotting code\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def plot_tsne_with_clusters_and_matches(model):\n",
    "    embeddings_2d = model_data[model]['embeddings_2d']\n",
    "    df = model_data[model]['df']\n",
    "    cluster_labels = model_data[model]['cluster_labels']\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "\n",
    "    # Plot all images colored by their cluster labels\n",
    "    scatter = plt.scatter(\n",
    "        embeddings_2d[:, 0], embeddings_2d[:, 1],\n",
    "        c=cluster_labels, cmap='tab10', s=10, alpha=0.6\n",
    "    )\n",
    "\n",
    "    # Highlight 'matches' images\n",
    "    matches_df = df[df['match_set'] != 'pubmed']\n",
    "    match_sets = matches_df['match_set'].unique()\n",
    "\n",
    "    # Assign different markers and edge colors to different match sets\n",
    "    markers = ['o', 's', '^', 'D', 'v', 'P', '*', 'X', 'h', '+']  # Extend if needed\n",
    "    edge_colors = ['black', 'blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink', 'gray', 'olive']\n",
    "    match_set_styles = {}\n",
    "    for i, match_set in enumerate(match_sets):\n",
    "        match_set_styles[match_set] = {\n",
    "            'marker': markers[i % len(markers)],\n",
    "            'edgecolor': edge_colors[i % len(edge_colors)]\n",
    "        }\n",
    "\n",
    "    # Plot 'matches' images with specific markers and edge colors\n",
    "    for match_set in match_sets:\n",
    "        indices = matches_df[matches_df['match_set'] == match_set].index\n",
    "        plt.scatter(\n",
    "            embeddings_2d[indices, 0], embeddings_2d[indices, 1],\n",
    "            facecolors='none',\n",
    "            edgecolors=match_set_styles[match_set]['edgecolor'],\n",
    "            marker=match_set_styles[match_set]['marker'],\n",
    "            s=80, linewidths=1.5,\n",
    "            label=f'Match Set {match_set}'\n",
    "        )\n",
    "\n",
    "    plt.title(f'{model} Embeddings t-SNE Plot with Clusters and Highlighted Match Sets')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    # Create legend for match sets\n",
    "    handles = []\n",
    "    for match_set in match_sets:\n",
    "        style = match_set_styles[match_set]\n",
    "        marker = plt.Line2D(\n",
    "            [], [], color=style['edgecolor'], marker=style['marker'],\n",
    "            linestyle='None', markersize=10, markerfacecolor='none',\n",
    "            label=f'Match Set {match_set}'\n",
    "        )\n",
    "        handles.append(marker)\n",
    "    plt.legend(handles=handles, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot for each model\n",
    "for model in models:\n",
    "    plot_tsne_with_clusters_and_matches(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aca025e-d450-4d84-9f44-f6bab022485b",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 6: Compare Retrieval Results Across Models**\n",
    "\n",
    "We will select a single query image from the \"matches\" set and compare the retrieval results across different models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c0a2c0-dff5-417a-8cac-2701da757044",
   "metadata": {},
   "source": [
    "**6.1. Select a Query Image from the \"Matches\" Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49ff22b-59c3-42ab-940a-2965b8dec963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display available images for querying from the 'matches' set\n",
    "matches_df = model_data[models[0]]['df']\n",
    "matches_images = matches_df[matches_df['match_set'] != 'pubmed']['image_file'].unique()\n",
    "\n",
    "print(\"Available images for querying from the 'matches' set:\")\n",
    "for idx, image_file in enumerate(matches_images):\n",
    "    print(f\"{idx}: {image_file}\")\n",
    "\n",
    "# Prompt the user to select an image index\n",
    "try:\n",
    "    query_idx = int(input(\"\\nEnter the index of the image you want to select: \"))\n",
    "    if query_idx < 0 or query_idx >= len(matches_images):\n",
    "        raise ValueError(\"Invalid index selected.\")\n",
    "    query_image_name = matches_images[query_idx]\n",
    "    print(f\"\\nSelected query image: {query_image_name}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}. Please enter a valid index.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1548eb-9852-4b23-9fb4-6fd0a21175d7",
   "metadata": {},
   "source": [
    "**6.2. Perform Retrieval and Display Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60867678-6d37-470b-9313-222f5540a9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_images_for_model(model, query_image_name, top_k=5):\n",
    "    embeddings = model_data[model]['embeddings']\n",
    "    image_files = model_data[model]['image_files']\n",
    "    df = model_data[model]['df']\n",
    "    \n",
    "    # Build the FAISS index\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    # Get the index of the query image\n",
    "    query_indices = df[df['image_file'] == query_image_name]['embedding_index'].tolist()\n",
    "    if not query_indices:\n",
    "        print(f\"No embeddings found for query image: {query_image_name} in model {model}\")\n",
    "        return None, None\n",
    "    query_idx = query_indices[0]\n",
    "    \n",
    "    # Perform the search\n",
    "    query_embedding = embeddings[query_idx].reshape(1, -1)\n",
    "    distances, indices = index.search(query_embedding, top_k + 1)  # +1 to include the query image itself\n",
    "    \n",
    "    # Get retrieved images\n",
    "    retrieved_indices = indices[0][1:]  # Exclude the query image itself\n",
    "    retrieved_images = [image_files[i] for i in retrieved_indices]\n",
    "    \n",
    "    # Evaluate retrieval\n",
    "    retrieved_df = df.iloc[retrieved_indices]\n",
    "    query_match_set = df.loc[query_idx, 'match_set']\n",
    "    correct_matches = retrieved_df[retrieved_df['match_set'] == query_match_set]\n",
    "    num_matches_found = len(correct_matches)\n",
    "    total_available_matches = len(df[df['match_set'] == query_match_set]) - 1  # Exclude the query image\n",
    "    match_accuracy = num_matches_found / total_available_matches if total_available_matches > 0 else 0\n",
    "    \n",
    "    return retrieved_images, match_accuracy\n",
    "\n",
    "# Retrieve images for each model\n",
    "retrieved_images_dict = {}\n",
    "match_accuracies = {}\n",
    "for model in models:\n",
    "    retrieved_images, match_accuracy = retrieve_images_for_model(model, query_image_name, top_k=15)\n",
    "    retrieved_images_dict[model] = retrieved_images\n",
    "    match_accuracies[model] = match_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae152ec-a227-495a-a4d4-a179bac78329",
   "metadata": {},
   "source": [
    "**6.3. Display Retrieval Results Side by Side**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d93d35-e0a9-47b3-ba73-8fbee9bd242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_retrievals_side_by_side(query_image_path, retrieved_image_paths_dict, match_accuracies):\n",
    "    num_models = len(retrieved_image_paths_dict)\n",
    "    num_retrieved = len(next(iter(retrieved_image_paths_dict.values())))\n",
    "    fig, axes = plt.subplots(num_retrieved + 1, num_models + 1, figsize=(4 * (num_models + 1), 4 * (num_retrieved + 1)))\n",
    "    \n",
    "    # Display the query image in the first column\n",
    "    query_img = Image.open(query_image_path)\n",
    "    axes[0, 0].imshow(query_img)\n",
    "    axes[0, 0].axis('off')\n",
    "    axes[0, 0].set_title('Query Image', fontsize=12)\n",
    "    \n",
    "    # Fill the rest of the query column with empty plots\n",
    "    for i in range(1, num_retrieved + 1):\n",
    "        axes[i, 0].axis('off')\n",
    "    \n",
    "    # Display retrieved images for each model\n",
    "    for col_idx, model in enumerate(retrieved_image_paths_dict.keys()):\n",
    "        # Add model name and match accuracy in the header row\n",
    "        axes[0, col_idx + 1].text(0.5, 0.5, f'{model}\\nMatch Accuracy: {match_accuracies[model]:.2f}', \n",
    "                                  fontsize=12, ha='center', va='center')\n",
    "        axes[0, col_idx + 1].axis('off')\n",
    "        \n",
    "        # Add retrieved images under the respective column\n",
    "        retrieved_paths = retrieved_image_paths_dict[model]\n",
    "        for row_idx, img_file in enumerate(retrieved_paths):\n",
    "            img_path = os.path.join(combined_dataset_dir, img_file)\n",
    "            img = Image.open(img_path)\n",
    "            axes[row_idx + 1, col_idx + 1].imshow(img)\n",
    "            axes[row_idx + 1, col_idx + 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Prepare the query image path\n",
    "query_image_path = os.path.join(combined_dataset_dir, query_image_name)\n",
    "\n",
    "# Display the retrieval results\n",
    "display_retrievals_side_by_side(query_image_path, retrieved_images_dict, match_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baf8d1f-af32-4cbf-821f-f8b64bf50ef0",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 7: Conclusion**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccea1a81-807f-4f2c-bbff-be5a3ba57d09",
   "metadata": {},
   "source": [
    "**[PLACEHOLDER]**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
