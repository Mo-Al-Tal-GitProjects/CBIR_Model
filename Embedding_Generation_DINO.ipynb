{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d74d8d75-a599-4db8-b170-54125a36ff44",
   "metadata": {},
   "source": [
    "## **Embedding Generation Using DINO (Self-Distillation with No Labels)**\n",
    "\n",
    "In this notebook, we will:\n",
    "- Load the pre-processed and augmented images.\n",
    "- Generate embeddings using the DINO model.\n",
    "- Save the embeddings and related data for evaluation..\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbdbbef-c1ca-4cb6-9bab-f638dc878c8c",
   "metadata": {},
   "source": [
    "## **Table of Contents**\n",
    "---\n",
    "1. Import Libraries\n",
    "2. Set Up Device\n",
    "3. Load Augmented Image Mapping\n",
    "4. Load DINO Model\n",
    "5. Generate Embeddings\n",
    "6. Select Representative Embeddings\n",
    "7. Save Embeddings\n",
    "8. Clear Memory\n",
    "9. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cccbba8-6c37-4452-8050-4278a262b005",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 1: Import Libraries**\n",
    "\n",
    "We begin by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a59abc5-063c-4e73-a12d-0a791d5110ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f295b16-a4dd-4219-8e7c-09767a8e61df",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 2: Set Up Device**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f440f50e-95e2-4165-b408-6d57c5b16de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac17ee0e-fd17-495d-a4e2-162516be278b",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 3: Load Augmented Image Mapping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a519874-3d34-46cf-9fe1-0f01c127fa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load augmented image mapping\n",
    "augmented_df = pd.read_csv('augmented_image_mapping.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8087a78-3e8e-4ee9-bdd4-868cbe446ad7",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 4: Load DINO Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a014959d-fa76-4ec9-9870-2b06aecdfcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/mohammedkhodorfirasal-tal/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    }
   ],
   "source": [
    "# Load the DINO model\n",
    "model = torch.hub.load('facebookresearch/dino:main', 'dino_vitb16').to(device)\n",
    "model.eval()\n",
    "\n",
    "# Define the transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.228, 0.224, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfffd97b-f3ff-45b9-bf06-c0a4ba6b6920",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 5: Generate Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3be2b7bd-6e96-46c7-9016-474e91e95b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 180/180 [38:42<00:00, 12.90s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 168  # Adjust based on your GPU memory\n",
    "embeddings = []\n",
    "original_image_files = []\n",
    "augmented_image_files = []\n",
    "\n",
    "# Directory containing augmented images\n",
    "augmented_dir = './augmented_dataset/'\n",
    "\n",
    "# Prepare a list of all image paths and corresponding original images\n",
    "image_paths = augmented_df['augmented_image'].apply(lambda x: os.path.join(augmented_dir, x)).tolist()\n",
    "original_images = augmented_df['original_image'].tolist()\n",
    "augmented_images = augmented_df['augmented_image'].tolist()\n",
    "\n",
    "# Total number of images\n",
    "total_images = len(image_paths)\n",
    "\n",
    "# Generate embeddings in batches\n",
    "for start_idx in tqdm(range(0, total_images, batch_size), desc='Generating Embeddings'):\n",
    "    end_idx = min(start_idx + batch_size, total_images)\n",
    "    batch_image_paths = image_paths[start_idx:end_idx]\n",
    "    batch_original_images = original_images[start_idx:end_idx]\n",
    "    batch_augmented_images = augmented_images[start_idx:end_idx]\n",
    "\n",
    "    # Load and preprocess images\n",
    "    imgs = [Image.open(p).convert('RGB') for p in batch_image_paths]\n",
    "    img_tensors = torch.stack([transform(img) for img in imgs]).to(device)\n",
    "\n",
    "    # Generate embeddings\n",
    "    with torch.no_grad():\n",
    "        batch_embeddings = model(img_tensors)\n",
    "        batch_embeddings = batch_embeddings.cpu().numpy()\n",
    "        # Normalize embeddings\n",
    "        batch_embeddings = batch_embeddings / np.linalg.norm(batch_embeddings, axis=1, keepdims=True)\n",
    "        embeddings.extend(batch_embeddings)\n",
    "        original_image_files.extend(batch_original_images)\n",
    "        augmented_image_files.extend(batch_augmented_images)\n",
    "\n",
    "    # Clear memory\n",
    "    del imgs, img_tensors, batch_embeddings\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Convert embeddings to NumPy array\n",
    "embeddings = np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b44ea8d-bcb6-430d-bd6e-2b8dfd30c6c9",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 6: Select Representative Embeddings**\n",
    "\n",
    "We will use the Highest Norm Criterion to select the most representative embedding for each original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ca509a1-7a4c-406c-bd44-4b29b233cf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for grouping\n",
    "data = pd.DataFrame({\n",
    "    'original_image': original_image_files,\n",
    "    'augmented_image': augmented_image_files,\n",
    "    'embedding_index': range(len(embeddings))\n",
    "})\n",
    "\n",
    "selected_embeddings = []\n",
    "selected_image_files = []\n",
    "\n",
    "# Group by original image and select the embedding with the highest norm\n",
    "for original_image, group in data.groupby('original_image'):\n",
    "    indices = group['embedding_index'].tolist()\n",
    "    group_embeddings = embeddings[indices]\n",
    "    # Compute norms of embeddings\n",
    "    norms = np.linalg.norm(group_embeddings, axis=1)\n",
    "    # Select the embedding with the highest norm\n",
    "    best_idx_in_group = np.argmax(norms)\n",
    "    best_idx = indices[best_idx_in_group]\n",
    "    selected_embeddings.append(embeddings[best_idx])\n",
    "    selected_image_files.append(original_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34d25f4-6515-4759-a9ca-d277a863fca9",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 7: Save Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a88be217-37c3-4214-990f-31842ad87070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings for DINO saved.\n"
     ]
    }
   ],
   "source": [
    "# Convert selected embeddings to NumPy array\n",
    "selected_embeddings = np.vstack(selected_embeddings)\n",
    "\n",
    "# Save embeddings and image files\n",
    "np.save('embeddings_dino.npy', selected_embeddings)\n",
    "np.save('image_files_dino.npy', selected_image_files)\n",
    "\n",
    "print('Embeddings for DINO saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1522ee72-849f-4c2b-92f6-a6f67aa662c7",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 8: Clear Memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adebd445-bcb6-441a-a680-d08c646c080a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear variables and free memory\n",
    "del embeddings, augmented_image_files, original_image_files, data\n",
    "del selected_embeddings, selected_image_files, model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f283ced-acf5-4134-a219-06cdbf9b4d2e",
   "metadata": {},
   "source": [
    "---\n",
    "### **Conclusion**\n",
    "\n",
    "We have generated embeddings using the DINO model, selected representative embeddings, and saved them for evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
